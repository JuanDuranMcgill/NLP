{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing and loading \"productions\" and \"uniqueProductions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running 2\n",
      "5000 out of 211968\n",
      "10000 out of 211968\n",
      "15000 out of 211968\n",
      "20000 out of 211968\n",
      "25000 out of 211968\n",
      "30000 out of 211968\n",
      "35000 out of 211968\n",
      "40000 out of 211968\n",
      "45000 out of 211968\n",
      "50000 out of 211968\n",
      "55000 out of 211968\n",
      "60000 out of 211968\n",
      "65000 out of 211968\n",
      "70000 out of 211968\n",
      "75000 out of 211968\n",
      "80000 out of 211968\n",
      "85000 out of 211968\n",
      "90000 out of 211968\n",
      "95000 out of 211968\n",
      "100000 out of 211968\n",
      "105000 out of 211968\n",
      "110000 out of 211968\n",
      "115000 out of 211968\n",
      "120000 out of 211968\n",
      "125000 out of 211968\n",
      "130000 out of 211968\n",
      "135000 out of 211968\n",
      "140000 out of 211968\n",
      "145000 out of 211968\n",
      "150000 out of 211968\n",
      "155000 out of 211968\n",
      "160000 out of 211968\n",
      "165000 out of 211968\n",
      "170000 out of 211968\n",
      "175000 out of 211968\n",
      "180000 out of 211968\n",
      "185000 out of 211968\n",
      "190000 out of 211968\n",
      "195000 out of 211968\n",
      "200000 out of 211968\n",
      "205000 out of 211968\n",
      "210000 out of 211968\n",
      "it took \n",
      "4514.873726844788\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk import Nonterminal\n",
    "from nltk import induce_pcfg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse import ViterbiParser\n",
    "from nltk import Production, nonterminals\n",
    "import re\n",
    "import time\n",
    "from functools import reduce\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"running\")\n",
    "productions=[]\n",
    "for item in treebank.fileids():\n",
    "    for tree in treebank.parsed_sents(item):\n",
    "          # perform optional tree transformations, e.g.:\n",
    "        tree.collapse_unary(collapsePOS = False)# Remove branches A-B-C into A-B+C\n",
    "        tree.chomsky_normal_form(horzMarkov = 2)# Remove A->(B,C,D) into A->B,C+D->D\n",
    "        productions+= tree.productions()\n",
    "    \n",
    "print(\"running 2\")\n",
    "ii=1\n",
    "t1=time.time()\n",
    "uniqueProductions = []\n",
    "frequence=[]\n",
    "for x in productions:\n",
    "    if x not in uniqueProductions:\n",
    "        uniqueProductions.append(x)\n",
    "        frequence.append(1)\n",
    "    else:\n",
    "        i=0\n",
    "        for p in uniqueProductions:       \n",
    "            if x == p:\n",
    "                frequence[i]+=1\n",
    "            i+=1\n",
    "    if ii%5000==0:\n",
    "        print(ii,\"out of\",len(productions))\n",
    "    ii+=1\n",
    "            \n",
    "        \n",
    "t2=time.time()\n",
    "print(\"it took \")\n",
    "print(t2-t1)\n",
    "\n",
    "def prodfreq(uniqueProductions,frequence,freq):   \n",
    "    pf = []\n",
    "    up=[]\n",
    "    for x,y in zip(uniqueProductions,frequence):       \n",
    "        if y>=freq:\n",
    "            up.append(x)\n",
    "            for f in range(y):\n",
    "                pf.append(x)\n",
    "    return pf,up\n",
    "\n",
    "prodfreq10,uprodfreq10 = prodfreq(uniqueProductions,frequence,10)\n",
    "\n",
    "prodfreq100,uprodfreq100 = prodfreq(uniqueProductions,frequence,100)\n",
    "\n",
    "prodfreq1000,uprodfreq1000 = prodfreq(uniqueProductions,frequence,1000)\n",
    "\n",
    "prodfreq10000,uprodfreq10000 = prodfreq(uniqueProductions,frequence,10000)\n",
    "\n",
    "\n",
    "prods = [productions,prodfreq10,prodfreq100,prodfreq1000,prodfreq10000]\n",
    "\n",
    "uprods=[uniqueProductions,uprodfreq10,uprodfreq100,uprodfreq1000,uprodfreq10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0 done\n",
      "f10 done\n",
      "f100 done\n",
      "f1000 done\n",
      "q5c done\n",
      "yea baby <3 enjoy life :P :*\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk import Nonterminal\n",
    "from nltk import induce_pcfg\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse import ViterbiParser\n",
    "from nltk import Production, nonterminals\n",
    "import re\n",
    "import time\n",
    "from functools import reduce\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "q5c()\n",
    "print(\"q5c done\")\n",
    "q5d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using \"uniqueProducitons\" to get all \"uniqueWordsInGrammar\" and \"uniqueNonTerminals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import nltk\n",
    "pat = \"-> \\'(.*?)\\'\"\n",
    "\n",
    "wordsInGrammar=[]\n",
    "nonTerminals=[]\n",
    "for x in uniqueProductions:\n",
    "    if re.search(pat,str(x)) is not None:\n",
    "        wordsInGrammar.append(re.search(pat,str(x)).group(1))\n",
    "        nonTerminals.append(str(x).split()[0])\n",
    "\n",
    "uniqueWordsInGrammar = []\n",
    "uniqueNonTerminals=[]\n",
    "for x,y in zip(wordsInGrammar,nonTerminals):\n",
    "    if x not in uniqueWordsInGrammar:\n",
    "        uniqueWordsInGrammar.append(x)\n",
    "    if y not in uniqueNonTerminals:\n",
    "        uniqueNonTerminals.append(y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using \"uniqueNonTerminals\" to append to \"productions\" new productions with word \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from nltk import Production, nonterminals\n",
    "for x in uniqueNonTerminals:\n",
    "    productions.append(nltk.grammar.Production(Nonterminal(x),[\"UNK\"]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making grammar with productions and setting up ViterParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "S = Nonterminal('S')\n",
    "grammar = induce_pcfg(S, productions)\n",
    "parser = ViterbiParser(grammar)\n",
    "parser.trace(3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing every unknown word in the dev.tsv document with \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tokensList=[]\n",
    "with open(\"dev.tsv\", \"r\") as a_file:\n",
    "    for line in a_file:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if line[2]==\"*\":\n",
    "            tokensList.append(word_tokenize(line[3]))\n",
    "    \n",
    "for tokens in tokensList:\n",
    "    i=0\n",
    "    for e in tokens:\n",
    "        if e not in uniqueWordsInGrammar:\n",
    "            tokens[i]=\"UNK\"\n",
    "        i+=1\n",
    "            \n",
    "'''     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. a) \n",
    "Pour resoudre le probleme des mots inconnus, je prends les productions et j'extrais les non-terminaux et tous les mots uniques. Ensuite, j'ajoute aux productions des nouvelles productions avec tous les genres de non-terminaux avec le mot \"UNK\".\n",
    "Apres, je compare les phrases de \"dev.tsv\" avec tous les mots uniques des productions pour trouver quels mots sont inconnus. Finalement, je remplace tous les mots inconnus par \"UNK\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. b)\n",
    "La longueur moyenne des phrases du corpus PTB pour entrainer une grammaire PCFG est de 25.72 (calculÃ© ci-dessous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.722023505365357\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "leavesPerTree=[]\n",
    "\n",
    "for item in treebank.fileids():\n",
    "    for tree in treebank.parsed_sents(item):\n",
    "          # perform optional tree transformations, e.g.:\n",
    "        tree.collapse_unary(collapsePOS = False)# Remove branches A-B-C into A-B+C\n",
    "        tree.chomsky_normal_form(horzMarkov = 2)# Remove A->(B,C,D) into A->B,C+D->D\n",
    "        leavesPerTree.append(len(tree.leaves()))\n",
    "print(statistics.mean(leavesPerTree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def q5c():\n",
    "\n",
    "    p0tokensList=[]\n",
    "    uprodsq= uprods[0:4]\n",
    "    prodsq = prods[0:4]\n",
    "\n",
    "    j=0\n",
    "    #J is outside\n",
    "    for upr,pr in zip(uprodsq,prodsq):\n",
    "\n",
    "        pat = \"-> \\'(.*?)\\'\"\n",
    "        wordsInGrammar=[]\n",
    "        nonTerminals=[]\n",
    "        for x in upr:\n",
    "            if re.search(pat,str(x)) is not None:\n",
    "                wordsInGrammar.append(re.search(pat,str(x)).group(1))\n",
    "                nonTerminals.append(str(x).split()[0])\n",
    "\n",
    "        uniqueWordsInGrammar = []\n",
    "        uniqueNonTerminals=[]\n",
    "        for x,y in zip(wordsInGrammar,nonTerminals):\n",
    "            if x not in uniqueWordsInGrammar:\n",
    "                uniqueWordsInGrammar.append(x)\n",
    "            if y not in uniqueNonTerminals:\n",
    "                uniqueNonTerminals.append(y)\n",
    "\n",
    "        for x in uniqueNonTerminals:\n",
    "            pr.append(Production(Nonterminal(x),[\"UNK\"]))\n",
    "\n",
    "        S = Nonterminal('S')\n",
    "        grammar = induce_pcfg(S, pr)\n",
    "        parser = ViterbiParser(grammar)\n",
    "        #parser.trace(3)\n",
    "\n",
    "        grammatical=[]\n",
    "\n",
    "        tokensList=[]\n",
    "        with open(\"dev.tsv\", \"r\") as a_file:\n",
    "            i=1\n",
    "            for line in a_file:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                if \"*\" in line[2]:\n",
    "                    grammatical.append(False)\n",
    "                else:\n",
    "                    grammatical.append(True)\n",
    "                tokensList.append(word_tokenize(line[3]))\n",
    "                i+=1\n",
    "                if i==300:\n",
    "                    break\n",
    "        for tokens in tokensList:\n",
    "            i=0\n",
    "            for e in tokens:\n",
    "                if e not in uniqueWordsInGrammar:\n",
    "                    tokens[i]=\"UNK\"\n",
    "                i+=1\n",
    "        num_unk=[]\n",
    "        num_words=[]\n",
    "        timet=[]\n",
    "        for tokens in tokensList:\n",
    "            nUnk=0\n",
    "            for e in tokens:\n",
    "                if e==\"UNK\":\n",
    "                    nUnk+=1\n",
    "            num_unk.append(nUnk)\n",
    "            num_words.append(len(tokens))\n",
    "\n",
    "        probability=[]\n",
    "        for tokens in tokensList:\n",
    "            t11=time.time()\n",
    "            p = reduce(lambda a,b:a+b.prob(), parser.parse(tokens), 0.0);\n",
    "            t22=time.time()\n",
    "            timet.append(t22-t11)\n",
    "            probability.append(p)\n",
    "\n",
    "\n",
    "        timet = pd.Series(timet)\n",
    "        num_unk= pd.Series(num_unk)\n",
    "        prob=probability\n",
    "        probabiliy=pd.Series(probability)\n",
    "        num_words=pd.Series(num_words)\n",
    "        grammatical=pd.Series(grammatical)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\"time in seconds\":timet,\"num_unk\":num_unk,\"num_words\":num_words,\"probability\":probability,\"grammatical\":grammatical})\n",
    "\n",
    "        if j==0:\n",
    "            df.to_csv(\"freq0.csv\") \n",
    "            for p in prob:\n",
    "                p0tokensList.append(tokensList[j])\n",
    "            print(\"f0 done\")\n",
    "        if j==1:\n",
    "            df.to_csv(\"freq10.csv\")  \n",
    "            print(\"f10 done\")\n",
    "        if j==2:\n",
    "            df.to_csv(\"freq100.csv\")  \n",
    "            print(\"f100 done\")\n",
    "        if j==3:\n",
    "            df.to_csv(\"freq1000.csv\")  \n",
    "            print(\"f1000 done\")\n",
    "        if j==4:\n",
    "            df.to_csv(\"freq10000.csv\")\n",
    "            print(\"f10000 done\")\n",
    "\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def q5d():\n",
    "    pr = prods[0]\n",
    "    upr = uprods[0]\n",
    "\n",
    "\n",
    "    pat = \"-> \\'(.*?)\\'\"\n",
    "    wordsInGrammar=[]\n",
    "    nonTerminals=[]\n",
    "    for x in upr:\n",
    "        if re.search(pat,str(x)) is not None:\n",
    "            wordsInGrammar.append(re.search(pat,str(x)).group(1))\n",
    "            nonTerminals.append(str(x).split()[0])\n",
    "\n",
    "    uniqueWordsInGrammar = []\n",
    "    uniqueNonTerminals=[]\n",
    "    for x,y in zip(wordsInGrammar,nonTerminals):\n",
    "        if x not in uniqueWordsInGrammar:\n",
    "            uniqueWordsInGrammar.append(x)\n",
    "        if y not in uniqueNonTerminals:\n",
    "            uniqueNonTerminals.append(y)\n",
    "\n",
    "    for x in uniqueNonTerminals:\n",
    "        pr.append(Production(Nonterminal(x),[\"UNK\"]))\n",
    "\n",
    "    S = Nonterminal('S')\n",
    "    grammar = induce_pcfg(S, pr)\n",
    "    parser = ViterbiParser(grammar)\n",
    "    #parser.trace(3)\n",
    "\n",
    "    tokensList=[]\n",
    "\n",
    "    with open(\"dev.tsv\", \"r\") as a_file:\n",
    "        for line in a_file:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line[2]==\"*\":\n",
    "                if len(word_tokenize(line[3])) <= 15:\n",
    "                    tokensList.append(word_tokenize(line[3]))\n",
    "    for tokens in tokensList:\n",
    "        i=0\n",
    "        for e in tokens:\n",
    "            if e not in uniqueWordsInGrammar:\n",
    "                tokens[i]=\"UNK\"\n",
    "            i+=1\n",
    "    num_unk=[]\n",
    "    num_words=[]\n",
    "    for tokens in tokensList:\n",
    "        nUnk=0\n",
    "        for e in tokens:\n",
    "            if e==\"UNK\":\n",
    "                nUnk+=1\n",
    "        num_unk.append(nUnk)\n",
    "        num_words.append(len(tokens))\n",
    "    timet=[]\n",
    "    probability=[]\n",
    "    for tokens in tokensList:\n",
    "        t11=time.time()\n",
    "        p = reduce(lambda a,b:a+b.prob(), parser.parse(tokens), 0.0);\n",
    "        t22=time.time()\n",
    "        timet.append(t22-t11)\n",
    "        probability.append(p)\n",
    "\n",
    "\n",
    "    timet = pd.Series(timet)\n",
    "    num_unk= pd.Series(num_unk)\n",
    "    probabiliy=pd.Series(probability)\n",
    "    num_words=pd.Series(num_words)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\"time in seconds\":timet,\"num_unk\":num_unk,\"num_words\":num_words,\"probability\":probability})\n",
    "\n",
    "    df.to_csv(\"5_d.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "pat = \"-> \\'(.*?)\\'\"\n",
    "\n",
    "wordsInGrammar=[]\n",
    "nonTerminals=[]\n",
    "for x in uniqueProductions:\n",
    "    if re.search(pat,str(x)) is not None:\n",
    "        wordsInGrammar.append(re.search(pat,str(x)).group(1))\n",
    "        nonTerminals.append(str(x).split()[0])\n",
    "\n",
    "uniqueWordsInGrammar = []\n",
    "uniqueNonTerminals=[]\n",
    "for x,y in zip(wordsInGrammar,nonTerminals):\n",
    "    if x not in uniqueWordsInGrammar:\n",
    "        uniqueWordsInGrammar.append(x)\n",
    "    if y not in uniqueNonTerminals:\n",
    "        uniqueNonTerminals.append(y)\n",
    "        \n",
    "        \n",
    "        \n",
    "S = Nonterminal('S')\n",
    "grammar = induce_pcfg(S, prods[0])\n",
    "parser = ViterbiParser(grammar)\n",
    "#parser.trace(3)\n",
    "\n",
    "tokensList=[]\n",
    "with open(\"dev.tsv\", \"r\") as a_file:\n",
    "    for line in a_file:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if \"*\" in line[2]:\n",
    "            tokensList.append(word_tokenize(line[3]))\n",
    "    \n",
    "for tokens in tokensList:\n",
    "    i=0\n",
    "    for e in tokens:\n",
    "        if e not in uniqueWordsInGrammar:\n",
    "            tokens[i]=\"UNK\"\n",
    "        i+=1\n",
    "            \n",
    "\n",
    "\n",
    "all_parses = {}\n",
    "\n",
    "parses = parser.parse_all(tokensList[0])\n",
    "\n",
    "for p in parses:\n",
    "        all_parses[p.freeze()] = 1\n",
    "        \n",
    "parses = all_parses.keys()\n",
    "\n",
    "from nltk.draw.tree import draw_trees\n",
    "\n",
    "draw_trees(*parses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparer resultats de 5.c) phrases grammaticales vs. agrammaticales et mentionner ce que les resultats des phrases agrammaticales on en commun. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
