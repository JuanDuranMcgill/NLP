{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "## Première approche\n",
    "1. Entraîner un modèle 4-gramme sur les 99 tranches du corpus avec KenLM (dans l'idéal on entraînerait 4 modèles, 1-2-3-4-gramme, pour analyser les résultats)\n",
    "2. Tokeniser toutes les phrases en entrée et créer des matrices avec toutes les permutations possibles pour chaque phrase (donc des matrices de taille (n_mots!, n_mots) )\n",
    "3. Prendre la permutation qui a le minimum de perplexité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import kenlm\n",
    "import util\n",
    "import models\n",
    "import math\n",
    "from tensorflow.keras.layers import Layer, Bidirectional, LSTM, Dropout, Embedding, Dense, Activation, ActivityRegularization, Lambda, Conv1D, MaxPooling1D, Flatten, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.python.keras import regularizers\n",
    "from itertools import permutations, combinations\n",
    "from util import SentVariations, SortSentenceV1, GetLeastPerplexSent\n",
    "from tokenizer import TextCleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour trouver toutes les permutations d'une phrase: *SentVariations*\n",
    "\n",
    "On remarque que lorsque le nombre de mots dépasse 6, le temps de traitement explose. Ceci est du au fait que cette solution cherche un nombre factoriel de permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = SentVariations(\"Your is daddy ? who\")\n",
    "for i in vars:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons la phrase avec le minimum de perplexité en utilisant notre model n-gramme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel('models/4gram.binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent, perp = SortSentenceV1(\"thank you mr segni , i shall do so gladly .\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted sentence: ' + ' '.join(sent))\n",
    "print ('perplexity: ' + str(round(perp,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Cette solution semble fonctionner pour des petites phrases, mais est impossible à traiter pour des phrases de taille plus grandes que 10 (*11 mots = 1 min de traitement*), telles que la plupart de nos phrases de test. \n",
    "\n",
    "## Deuxième approche\n",
    "\n",
    "Calculer n! permutations prend un temps monstrueux lorsque n>10. \n",
    "1. Considérer que chaque mot de la phrase peut être le premier mot\n",
    "2. Pour chaque mot, utiliser le modèle pour prédire le mot suivant le plus probable considérant la phrase d'entrée et la liste des mots restants à deviner (~ Greedy Search)\n",
    "3. On obtient une matrice de forme (n_mots, n_mots), il suffit ensuite de trouver la phrase la moins perplèxe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneStepPrediction(sent, words, model):\n",
    "    \"\"\"\n",
    "    Performs one prediction step. Takes the input incomplete sentence, and predicts the next most probable word not previously drawn\n",
    "\n",
    "    Arguments:\n",
    "        sent -- str, the input sentence\n",
    "        words -- the list of remaining words to be sorted\n",
    "        model -- the KenLM model instance\n",
    "    \n",
    "    Returns: \n",
    "        newSent -- str, the completed sentence\n",
    "        words -- the remaining words to be assigned\n",
    "    \"\"\" \n",
    "    \n",
    "    perplexity = model.perplexity(sent + ' ' + words[0])\n",
    "    \n",
    "    for word in words:\n",
    "        sentCandidate = sent + ' ' + word\n",
    "        perpCandidate = model.perplexity(sentCandidate)\n",
    "        if perpCandidate <= perplexity:\n",
    "            perplexity = perpCandidate\n",
    "            newSent = sentCandidate\n",
    "            chosenWord = word\n",
    "    words.remove(chosenWord)\n",
    "    \n",
    "    return newSent, words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortSentenceV2 (words, model):\n",
    "    \"\"\"\n",
    "    Predict a sentence from a list of words, using a previsouly trained KenLM n-gram model\n",
    "\n",
    "    Arguments:\n",
    "        words -- the list of words to place in order  \n",
    "        model -- the KenLM n-gram model  \n",
    "    \n",
    "    Returns:\n",
    "        outSent -- the output sentence  \n",
    "        perplexity -- the perplexity of the output sentence\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    \n",
    "    # Every word can be the starting word\n",
    "    for word in words:        \n",
    "        remainingWords = words.copy()\n",
    "        remainingWords.remove(word)\n",
    "        sent = word\n",
    "        \n",
    "        # While there is still words to be sorted, perform one prediction step\n",
    "        while len(remainingWords) > 0:\n",
    "            sent, remainingWords = OneStepPrediction(sent, remainingWords, model)\n",
    "        sents.append(sent)\n",
    "        \n",
    "\n",
    "    # Get the sentence with the minimum of perplexity from the list of sentences\n",
    "    outSent, perplexity = GetLeastPerplexSent(sents, model)\n",
    "    \n",
    "    return outSent, perplexity, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"kitchen is daddy the with cleaning mama .\"\n",
    "words = sent.split()\n",
    "outSent, perplexity, sents = SortSentenceV2(words, model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('out: ' + outSent)\n",
    "print('perplexity: ' + str(perplexity))\n",
    "for sent in sents :\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que notre système place systématiquement le '.' en deuxième positionm plutôt qu'à la fin.  \n",
    "Voyons voir ce qu'il se passe si l'on prend les 4-grammes les plus probables commençant par chaque mot du vocabulaire, plutôt que le mot unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNgrams(words, model, o = 4):\n",
    "    \"\"\"\n",
    "    Gets the most probable n-gram for each word in a list\n",
    "    Arguments:\n",
    "        words -- the list of words\n",
    "        model -- the KenLM n-gram model\n",
    "        o -- int, order of the n-gram (default: 4)\n",
    "\n",
    "    Returns:\n",
    "        ngrams -- list of str, all the most probable n-grams for each word of the dictionnary\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    ngrams_current = []\n",
    "\n",
    "    for word in words:\n",
    "        # Get all possible permutations of order o\n",
    "        perms = permutations(words, o)\n",
    "        ngrams_current.clear()\n",
    "        \n",
    "        # For each word, get the permutations starting by it\n",
    "        for perm in perms:            \n",
    "            if str(word) == str(perm[0]):\n",
    "                ngrams_current.append(' '.join(perm))\n",
    "        # Get the least perplex ngram\n",
    "        ngram, _ = GetLeastPerplexSent(ngrams_current, model)\n",
    "        ngrams.append(ngram)\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"to , attention should parliament like your an a i case to which president . shown draw interest consistently in this has madam\".split()\n",
    "ngrams = GetNgrams(words, model, o = 4)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que TOUS nos n-grammes terminent par un point '.', ce qui peut être problématique puisqu'il est plus probable qu'un point termine notre phrase plutôt qu'être au début.  \n",
    "Pour s'affranchir de ce problème, prenons les n-1 mots des n-grammes pour commencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortSentenceV3 (words, model):\n",
    "    \"\"\"\n",
    "    Predict a sentence from a list of words, using a previsouly trained KenLM n-gram model\n",
    "\n",
    "    Arguments:\n",
    "        words -- the list of words to place in order  \n",
    "        model -- the KenLM n-gram model  \n",
    "    \n",
    "    Returns:\n",
    "        outSent -- the output sentence  \n",
    "        perplexity -- the perplexity of the output sentence\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    \n",
    "    # Every word can be the starting word\n",
    "    for ngram in GetNgrams(words, model):        \n",
    "        ngram = ngram.split()\n",
    "        remainingWords = words.copy()\n",
    "        # If the ngram ends with '.', pop the end\n",
    "        if ngram[-1] == '.':\n",
    "            ngram.pop()\n",
    "        for word in ngram:\n",
    "            remainingWords.remove(word)\n",
    "        sent = ' '.join(ngram)\n",
    "        \n",
    "        # While there is still words to be sorted, perform one prediction step\n",
    "        while len(remainingWords) > 0:\n",
    "            sent, remainingWords = OneStepPrediction(sent, remainingWords, model)\n",
    "        sents.append(sent)\n",
    "        \n",
    "\n",
    "    # Get the sentence with the minimum of perplexity from the list of sentences\n",
    "    outSent, perplexity = GetLeastPerplexSent(sents, model)\n",
    "    \n",
    "    return outSent, perplexity, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"my question relates to something that will come up on thursday and which i will then raise again .\".split()\n",
    "outSent, perplexity, sents = SortSentenceV3(words, model)   \n",
    "print('out: ' + outSent)\n",
    "print('perplexity: ' + str(perplexity))\n",
    "for sent in sents :\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilan\n",
    "\n",
    "On remarque que le premier mot prédit est souvent le '.', les virgules etc. Ce n'est pas très surprenant puisque ce sont les mots de vocabulaire les plus courants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troisième approche: Modèles Seq2Seq\n",
    "1. On entraîne un modèle séquence-à-séquence par la méthode du \"Teacher Forcing\"\n",
    "2. Encodeur: LSTM / BiLSTM / CNN, avec ou sans couche d'embeddings\n",
    "3. Décodeur: LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données\n",
    "#### 1- Chargement des dataset (50 000 phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "['<bos> big issue does everything become have a to why ? such <eos>'\n",
      " '<bos> if most the in mid-october contenders candidate runoff , absolute be between a no top . there wins likely will an two majority , <eos>'\n",
      " \"<bos> his warrants search vegas served and . at 's las home businesses murray and houston authorities previously las vegas in <eos>\"]\n",
      "Y_train:\n",
      "['<bos> why does everything have to become such a big issue ? <eos>'\n",
      " '<bos> if no candidate wins an absolute majority , there will be a runoff between the top two contenders , most likely in mid-october . <eos>'\n",
      " \"<bos> authorities previously served search warrants at murray 's las vegas home and his businesses in las vegas and houston . <eos>\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = util.GetData('data\\\\1BW.train', 'data\\\\1BW.ref', 'data\\\\devdata.test', 'data\\\\devdata.ref', 1000)\n",
    "print('X_train:')\n",
    "print(X_train[:3])\n",
    "print('Y_train:')\n",
    "print(Y_train[:3])\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Chargement des embeddings et du dictionnaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict, word_to_index, index_to_word = util.LoadVectors('data\\\\glove_small.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Préparation des données sous forme de liste d'entiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000\n"
     ]
    }
   ],
   "source": [
    "maxLen = 25 + 2 \n",
    "X_train_int = util.Sentences2Indices(X_train, word_to_index, maxLen)\n",
    "Y_train_int = util.Sentences2Indices(Y_train, word_to_index, maxLen)\n",
    "X_test_int = util.Sentences2Indices(X_test, word_to_index, maxLen)\n",
    "Y_test_int = util.Sentences2Indices(Y_test, word_to_index, maxLen)\n",
    "\n",
    "print(X_train_int.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'big', 'issue', 'does', 'everything', 'become', 'have', 'a', 'to', 'why', '?', 'such', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for i in X_train_int[0]:\n",
    "    test.append(index_to_word[i])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400004\n",
      "27000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 80.5 GiB for an array with shape (27000, 400004) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11372/3679613237.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train_oh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\UDEM\\IFT6285\\Projet2\\util.py\u001b[0m in \u001b[0;36mOneHot\u001b[1;34m(values, number_of_classes)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \"\"\"\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mone_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mone_hot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 80.5 GiB for an array with shape (27000, 400004) and data type float64"
     ]
    }
   ],
   "source": [
    "vocabSize = len(word_to_index.keys())\n",
    "print(vocabSize)\n",
    "print(X_train_int.size)\n",
    "X_train_oh = util.OneHot(X_train_int, vocabSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateBatch(X, Y, vocabSize, batch_size=64, maxLen = 27):\n",
    "    ''' \n",
    "    Generate a batch of data \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_inputs = np.zeros((batch_size, maxLen),dtype='float32')\n",
    "            decoder_inputs = np.zeros((batch_size, maxLen+2),dtype='float32')\n",
    "            decoder_outputs = np.zeros((batch_size, maxLen+2, vocabSize),dtype='float32')\n",
    "\n",
    "            for i, (input_text_seq, target_text_seq) in enumerate(zip(X[j:j+batch_size], Y[j:j+batch_size])):\n",
    "                for t, word_index in enumerate(input_text_seq):\n",
    "                    encoder_inputs[i, t] = word_index # encoder input seq\n",
    "\n",
    "                for t, word_index in enumerate(target_text_seq):\n",
    "                    decoder_inputs[i, t] = word_index\n",
    "                    if (t>0) and (word_index<=vocabSize):\n",
    "                        decoder_outputs[i, t-1, int(word_index-1)] = 1.\n",
    "\n",
    "            yield([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = GenerateBatch(X_train_int, Y_train_int, vocabSize,64,27)\n",
    "for a in A:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implémentation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testSeq2Seq(Model):\n",
    "    \"\"\"\n",
    "    Architecture for the sequence-to-sequence Model used in the linearization task.\n",
    "    This model is comprised of an encoder and a decoder, and uses teacher forcing for training.\n",
    "\n",
    "    The encoder type can be chosen via the 'encoder_type' argument. There are 3 types available:\n",
    "        'lstm' -- a simple LSTM RNN many-to-one\n",
    "        'bilstm' -- a BiLSTM RNN\n",
    "        'CNN' -- a simple 1D CNN\n",
    "    \n",
    "    the encoder outputs and states can be accessed after training via the 'encoder_outputs' and 'encoder_states' properties\n",
    "\n",
    "    Arguments:\n",
    "        max_words -- int, the maximum size, in words, of the sentences/bags of words (default=25)\n",
    "        embedding_dict -- the dictionnary mapping every word to its vectorial representation\n",
    "        word_to_index -- the dictionnary mapping every word in the vocabulary to its index (used in the embedding layer)\n",
    "        encoder_type -- string, 'lstm' 'bilstm' or 'cnn'. Specifies the type of encoder to be used\n",
    "        encoder_lstm_units -- int, number of hidden units in the (bi)lstm layers of the encoder (default=64)\n",
    "        decoder_units -- int, number of hidden units in the lstm layer of the decoder (default=64)\n",
    "        name -- string, name of the model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dict,\n",
    "        word_to_index,\n",
    "        max_words=27,\n",
    "        encoder_type='lstm',\n",
    "        n_a=32,\n",
    "        n_s=64,\n",
    "        name=\"seq2seq\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(testSeq2Seq, self).__init__(name=name, **kwargs)\n",
    "        self.inputs = [Input((max_words,)), Input((max_words,))]\n",
    "\n",
    "        # Check the desired type of encoding\n",
    "        if encoder_type == 'bilstm':\n",
    "            self.encoder = models.EncoderBiLSTM()\n",
    "            self.encoder_type = 'bilstm'\n",
    "        elif encoder_type == 'cnn':\n",
    "            #TODO: Add the CNN Encoder here\n",
    "            None\n",
    "        else:\n",
    "            self.encoder = EncoderLSTM(embedding_dict,word_to_index,n_a)\n",
    "            self.encoder_type = 'lstm'\n",
    "\n",
    "        self.decoder = DecoderLSTM(embedding_dict, word_to_index, n_s)\n",
    "    \n",
    "    def call(self, inputs):        \n",
    "        encoder_outputs, encoder_states = self.encoder(inputs[0])\n",
    "        X = self.decoder(inputs[1], encoder_states)\n",
    "        \n",
    "        ## Save values for inference ##\n",
    "        self.encoder_outputs = encoder_outputs\n",
    "        self.encoder_states = encoder_states\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917498"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove32 = {}\n",
    "for key in glove_dict:\n",
    "    glove32[key] = np.float32(glove_dict[key])\n",
    "len(glove32.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(Layer):\n",
    "    def __init__(self, embedding_dict, word_to_index, n_a = 32, **kwargs):\n",
    "        super(EncoderLSTM, self).__init__()        \n",
    "        self.embedding = models.EmbeddingLayer(embedding_dict,word_to_index)\n",
    "        self.lstm = LSTM(n_a, return_sequences=False, return_state=True)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        encoder_embeddings = self.embedding(inputs)\n",
    "        X, state_h, state_c = self.lstm(encoder_embeddings)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        print(encoder_states)\n",
    "        return X, encoder_states\n",
    "\n",
    "class DecoderLSTM(Layer):\n",
    "    def __init__(self, embedding_dict, word_to_index, n_s = 64, dropout_rate=0.5, **kwargs):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        vocabSize = len(word_to_index.keys())\n",
    "        self.lstm = LSTM(n_s, return_sequences=True, return_state=True)\n",
    "        self.embedding = models.EmbeddingLayer(embedding_dict,word_to_index)\n",
    "        self.dense = Dense(vocabSize, activation='softmax')\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, encoder_states, training=None):\n",
    "        decoder_embeddings = self.embedding(inputs)\n",
    "        X, state_h, state_c = self.lstm(decoder_embeddings, initial_state=encoder_states)\n",
    "        X = self.dropout(X)\n",
    "        X = self.dense(X)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        return X, decoder_states\n",
    "\n",
    "def Seq2Seq(maxLen, n_a, n_s, embedding_dict, word_to_index):\n",
    "    \"\"\"\n",
    "    Architecture for the sequence-to-sequence Model used in the linearization task.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs\n",
    "    encoder_inputs = Input(shape=(maxLen,))\n",
    "    # h0 = Input(shape=(n_s,), name='s0')\n",
    "    # c0 = Input(shape=(n_s,), name='c0')\n",
    "    decoder_inputs = Input((maxLen,))\n",
    "\n",
    "    # Encoder\n",
    "    encoder_embeddings = models.EmbeddingLayer(embedding_dict,word_to_index)(encoder_inputs)\n",
    "    encoder_outputs, state_h, state_c = LSTM(n_a, return_sequences=False, return_state=True)(encoder_embeddings)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    # encoder_outputs, encoder_states = EncoderLSTM(embedding_dict,word_to_index,n_a)(encoder_inputs)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_embeddings = models.EmbeddingLayer(embedding_dict,word_to_index)(decoder_inputs)\n",
    "    X, _, _ = LSTM(n_s, return_sequences=True, return_state=True)(decoder_embeddings, initial_state=encoder_states)\n",
    "    X = Dense(vocabSize, activation='softmax')(X)\n",
    "    # X = DecoderLSTM(embedding_dict=embedding_dict, word_to_index=word_to_index, n_s=n_s)(decoder_inputs, encoder_states)\n",
    "       \n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=X)\n",
    "    return model, encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 25 + 2\n",
    "\n",
    "# embedding_dict=glove_dict, word_to_index=word_to_index\n",
    "baseline_model = testSeq2Seq(glove_dict,word_to_index,27,'lstm', 64,64)\n",
    "baseline_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_lstm (EncoderLSTM)  multiple                  0 (unused)\n",
      "                                                                 \n",
      " decoder_lstm (DecoderLSTM)  multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,000,500\n",
      "Trainable params: 0\n",
      "Non-trainable params: 40,000,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model.build((maxLen,))\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "vocabSize = len(word_to_index.keys())\n",
    "baseline_model.fit_generator(\n",
    "    generator=models.GenerateBatch(X_train_int, Y_train_int, vocabSize, batch_size),\n",
    "    steps_per_epoch=math.ceil(len(X_train_int)/batch_size),\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=models.GenerateBatch(X_test_int, Y_test_int, vocabSize, batch_size=batch_size),\n",
    "    validation_steps=math.ceil(len(X_test_int)/batch_size),\n",
    "    workers=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models\\\\seq2seq_bilstm2021-12-18_10-55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderCNN(embedding_dict, word_to_index, n_s, dropout_rate=0.5):\n",
    "    # Define the inputs\n",
    "    encoder_inputs = Input(shape=(maxLen,))\n",
    "\n",
    "    encoder_embeddings = models.EmbeddingLayer(embedding_dict,word_to_index)(encoder_inputs)\n",
    "    X = Conv1D(1024,3, padding='same', activation='relu')(encoder_embeddings)\n",
    "    X = Conv1D(512,3, padding='same', activation='relu')(X)\n",
    "    X = MaxPooling1D()(X)\n",
    "    X = Conv1D(256,3, padding='same', activation='relu')(X)\n",
    "    X = MaxPooling1D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(dropout_rate)(X)\n",
    "    state_c = Dense(n_s, activation='linear')(X)\n",
    "    state_h = Dense(n_s, activation='linear')(X)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    model = Model(inputs=encoder_inputs, outputs=[state_c, encoder_states])\n",
    "\n",
    "    return model, state_c, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cnn, _, _ = EncoderCNN(glove_dict, word_to_index, n_s = 64)\n",
    "model_cnn=models.Seq2Seq(glove_dict, word_to_index, 27, 'cnn', 64, 64)\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_cnn (EncoderCNN)    multiple                  0 (unused)\n",
      "                                                                 \n",
      " decoder_lstm (DecoderLSTM)  multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,000,500\n",
      "Trainable params: 0\n",
      "Non-trainable params: 40,000,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.build((maxLen,))\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_16916/1944083657.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_cnn.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15/125 [==>...........................] - ETA: 58s - loss: 11.4636"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16916/1944083657.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvocabSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model_cnn.fit_generator(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGenerateBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2014\u001b[0m         \u001b[1;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2015\u001b[0m         stacklevel=2)\n\u001b[1;32m-> 2016\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   2017\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocabSize = len(word_to_index.keys())\n",
    "model_cnn.fit_generator(\n",
    "    generator= models.GenerateBatch(X_train_int, Y_train_int, vocabSize, 8),\n",
    "    steps_per_epoch= math.ceil(len(X_train_int)/8),\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    validation_data= models.GenerateBatch(X_test_int, Y_test_int, vocabSize, batch_size=8),\n",
    "    validation_steps=math.ceil(len(X_test_int)/8),\n",
    "    workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = models.Seq2Seq(glove_dict, word_to_index, maxLen, 'lstm', 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_lstm_1 (EncoderLSTM  multiple                 0 (unused)\n",
      " )                                                               \n",
      "                                                                 \n",
      " decoder_lstm_2 (DecoderLSTM  multiple                 0 (unused)\n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,000,500\n",
      "Trainable params: 0\n",
      "Non-trainable params: 40,000,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.build((maxLen,))\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_16008/905782672.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_lstm.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 189s 2s/step - loss: 8.0028 - accuracy: 0.2835 - val_loss: 4.9949 - val_accuracy: 0.3104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ce695ba30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSize = len(word_to_index.keys())\n",
    "model_lstm.fit_generator(\n",
    "    generator= models.GenerateBatch(X_train_int, Y_train_int, vocabSize, 8),\n",
    "    steps_per_epoch= math.ceil(len(X_train_int)/8),\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    validation_data= models.GenerateBatch(X_test_int, Y_test_int, vocabSize, batch_size=8),\n",
    "    validation_steps=math.ceil(len(X_test_int)/8),\n",
    "    workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model_lstm.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSamplingModels(model, n_s, maxLen, embedding_dict, word_to_index):\n",
    "    vocabSize = len(word_to_index.keys())\n",
    "    encoder_inputs = Input((maxLen,))\n",
    "    _, encoder_states = model.encoder(encoder_inputs)\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_inputs = Input((maxLen,))\n",
    "    decoder_state_input_h = Input(shape=(n_s,))\n",
    "    decoder_state_input_c = Input(shape=(n_s,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_embeddings = models.EmbeddingLayer(embedding_dict,word_to_index)(decoder_inputs)\n",
    "    X, state_h, state_c = LSTM(n_s, return_sequences=True, return_state=True)(decoder_embeddings, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = Dense(vocabSize, activation='softmax')(X)\n",
    "    \n",
    "    decoder_model = Model([decoder_inputs, decoder_states_inputs], [decoder_outputs, decoder_states])\n",
    "    \n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = GetSamplingModels(model_lstm, 64, maxLen, glove_dict, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_states = encoder_model.predict(X_test_int[0].reshape(1,27))\n",
    "np.array(enc_states)[0].shape\n",
    "np.array(word_to_index['<bos>']).reshape(1,1).shape\n",
    "a = [np.array(word_to_index['<bos>']).reshape(1,1), enc_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 400004)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, [h, c] = decoder_model.predict(a)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_52 (InputLayer)          [(None, 27)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_15 (Embedding)       (None, 27, 50)       20000250    ['input_52[0][0]']               \n",
      "                                                                                                  \n",
      " input_53 (InputLayer)          [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_54 (InputLayer)          [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 [(None, 27, 64),     29440       ['embedding_15[0][0]',           \n",
      "                                 (None, 64),                      'input_53[0][0]',               \n",
      "                                 (None, 64)]                      'input_54[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 27, 400004)   26000260    ['lstm_14[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,029,950\n",
      "Trainable params: 26,029,700\n",
      "Non-trainable params: 20,000,250\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodeSequence(input_seq, encoder_model, decoder_model, word_to_index, index_to_word, maxLen):\n",
    "    vocabSize = len(word_to_index.keys())\n",
    "    # Encode the input as state vectors.\n",
    "    input_seq = input_seq.reshape(1,maxLen).astype(np.int32)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq = np.array(word_to_index['<bos>']).reshape(1,1)\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, states_value = decoder_model.predict([target_seq,  states_value])\n",
    "        # Sample a token\n",
    "        sampled_word_seq_index = np.argmax(output_tokens[:,:,input_seq.astype(np.int32)[0]], axis = 2) # index of the word in the input sequence voc\n",
    "        sampled_word_index = input_seq[0,sampled_word_seq_index[0][0]] # get the vocabulary index of the found word\n",
    "        input_seq = np.delete(input_seq, sampled_word_seq_index, axis = 1) # removes the word from the input sentence\n",
    "        sampled_word = index_to_word[sampled_word_index] # get the corresponding word in the vocabulary\n",
    "        decoded_sentence.append(sampled_word)\n",
    "\n",
    "\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<eos>' or len(decoded_sentence) > maxLen or input_seq.size < 1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1). \n",
    "        target_seq = np.array([[sampled_word_index]])\n",
    "        # target_seq = np.append(target_seq, [sampled_word_index], axis=1)\n",
    "\n",
    "        # Update states\n",
    "        states_value = states_value\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneStepBeamSearch(decoder_model, word, score, states, seq_voc, k):\n",
    "    out_words = []\n",
    "    out_scores = []\n",
    "    out_states = []\n",
    "    out_vocabs = []\n",
    "    \n",
    "    output_probs, output_states = decoder_model.predict([word, states]) # next word probs\n",
    "    output_scores = tf.math.log(output_probs) # next word log(probs)\n",
    "    seq_voc_out_scores = np.array(output_scores)[:,:,seq_voc.astype(np.int32)][0,0,:] # only consider words in the seq vocab\n",
    "    best_k_next_in_seq_voc = np.argpartition(seq_voc_out_scores, -k)[-k:] # take the k best choices\n",
    "    # for each beam:\n",
    "    #   create a new seq with appended new index\n",
    "    #   Compute new score\n",
    "    #   Remove word from seq vocab\n",
    "    for i in range(k):\n",
    "        sampled_word_seq_index = best_k_next_in_seq_voc[i]\n",
    "        sampled_word_index = seq_voc[sampled_word_seq_index]\n",
    "        new_score = score + seq_voc_out_scores[sampled_word_seq_index]\n",
    "        out_words.append(np.array([[sampled_word_index]]))\n",
    "        out_scores.append(new_score)\n",
    "        out_vocabs.append(np.array(np.delete(seq_voc, sampled_word_seq_index)))\n",
    "        out_states.append(output_states)\n",
    "\n",
    "    return out_words, out_scores, out_states, out_vocabs\n",
    "\n",
    "def BeamSearch(decoder_model, branches, branchesScores, thisBranch, init_word, init_score, init_states, init_vocab, k = 1):\n",
    "    \n",
    "    # Check if there is enough remaining words\n",
    "    if init_vocab.size <= k:\n",
    "        k = init_vocab.size\n",
    "    out_words, out_scores, out_states, out_vocabs =  OneStepBeamSearch(decoder_model, init_word, init_score, init_states, init_vocab, k)\n",
    "    for i in range(k):\n",
    "        thisBranch.extend(out_words[i])\n",
    "        thisBranchScore = out_scores[i]\n",
    "        if init_vocab.size < 2:\n",
    "            branches.append(thisBranch)\n",
    "            branchesScores.append(thisBranchScore)\n",
    "            break\n",
    "        else:\n",
    "            branches, branchesScores = BeamSearch(decoder_model, branches, branchesScores, thisBranch, out_words[i], thisBranchScore, out_states[i], out_vocabs[i], k)\n",
    "\n",
    "    return branches, branchesScores\n",
    "    \n",
    "\n",
    "def DecodeSequenceV2(input_seq, encoder_model, decoder_model, word_to_index, maxLen, k = 1):\n",
    "    realLen = input_seq[input_seq != 0].size\n",
    "    beams = []\n",
    "    scores = []\n",
    "    init_word = np.array(word_to_index['<bos>']).reshape(1,1)\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    input_seq = input_seq.reshape(1,maxLen).astype(np.int32)\n",
    "    states_values = encoder_model.predict(input_seq)\n",
    "    \n",
    "    beams, scores = BeamSearch(decoder_model, beams, [], [], init_word, 1, states_values, input_seq[input_seq != 0], k)\n",
    "    return beams[np.argmax(scores)], beams, scores\n",
    "\n",
    "def Sequence2Sentence(seq, index_to_word):\n",
    "    sent = ''\n",
    "    for n in seq:\n",
    "        if n[0] != 0:\n",
    "            sent = sent + ' ' + index_to_word[n[0]]\n",
    "    return sent.replace(\" <eos>\", '').replace(\" <bos>\", '').strip()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = load_model('models\\\\seq2seq_bilstm2021-12-18_10-55')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = models.GetSamplingModels(model_lstm, 64, maxLen, glove_dict, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(X_test_int[0].size)\n",
    "realLen =X_test_int[0][X_test_int[0]!=0].size\n",
    "print(realLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = X_test_int[0]\n",
    "maxLen = 27\n",
    "glove_dict, word_to_index, index_to_word = util.LoadVectors('data/glove_small.txt')\n",
    "model = load_model('petitModele')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 27) for input KerasTensor(type_spec=TensorSpec(shape=(None, 27), dtype=tf.float32, name='input_440'), name='input_440', description=\"created by layer 'input_440'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "Decoded Sentence : \"s ' this then , , minute . please for silence rise\"\n",
      "\n",
      "Beams: \n",
      "\n",
      "-179.56203174591064\ts ' this then , , minute . please for silence rise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_seq = X_test_int[0]\n",
    "\n",
    "encoder_model, decoder_model = models.GetSamplingModels(model, 64, maxLen, glove_dict, word_to_index)\n",
    "seq, beams, scores = DecodeSequenceV2(input_seq, encoder_model, decoder_model, word_to_index, maxLen, k = 1)\n",
    "\n",
    "print('Decoded Sentence : \"' + Sequence2Sentence(seq, index_to_word) + '\"\\n')\n",
    "print('Beams: \\n')\n",
    "for i in range(len(beams)):\n",
    "    print(str(scores[i]) + '\\t' + Sequence2Sentence(beams[i], index_to_word) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
